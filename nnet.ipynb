{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.layers import Layer\n",
    "from lasagne.random import get_rng\n",
    "\n",
    "from agentnet import Recurrence\n",
    "from agentnet.resolver import ProbabilisticResolver\n",
    "from agentnet.memory import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'log'\n",
    "log_filename = 'data/'+filename+'.csv'\n",
    "json_filename = 'output/' + filename + \".json\"\n",
    "dot_filename = 'output/' + filename + \".dot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GumbelSigmoid:\n",
    "    def __init__(self,\n",
    "                 t=0.1,\n",
    "                 discrete=True,\n",
    "                 eps=1e-20):\n",
    "        assert t != 0\n",
    "        self.temperature=t\n",
    "        self.eps=eps\n",
    "        self.discrete=discrete\n",
    "        self._srng = RandomStreams(get_rng().randint(1, 2147462579))\n",
    "    def __call__(self,logits):\n",
    "        \"\"\"computes a gumbel softmax sample\"\"\"\n",
    "                \n",
    "        #sample from Gumbel(0, 1)\n",
    "        uniform1 = self._srng.uniform(logits.shape,low=0,high=1)\n",
    "        uniform2 = self._srng.uniform(logits.shape,low=0,high=1)\n",
    "        \n",
    "        noise = -T.log(T.log(uniform2 + self.eps)/T.log(uniform1 + self.eps) +self.eps)\n",
    "        \n",
    "        #draw a sample from the Gumbel-Sigmoid distribution\n",
    "        gumbel_sigm = T.nnet.sigmoid((logits + noise) / self.temperature)\n",
    "        \n",
    "        if self.discrete:\n",
    "            return theano.gradient.zero_grad(hard_sigm(logits + noise) - gumbel_sigm) + gumbel_sigm\n",
    "        else:\n",
    "            return gumbel_sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hard_sigm(logits):\n",
    "    \"\"\"computes a hard indicator function. Not differentiable\"\"\"\n",
    "    return T.switch(T.gt(logits,0),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_log(filename, sep=';', sheetname='Лист1'):\n",
    "    if filename.lower().endswith('.csv'):\n",
    "        df = pd.read_csv(filename,sep=sep)\n",
    "    elif filename.lower().endswith('.xlsx'):\n",
    "        xl = pd.ExcelFile(filename)\n",
    "        df = xl.parse(sheetname)\n",
    "    else:\n",
    "        raise Exception(\"Некорректный тип лога\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(filename):\n",
    "    df = read_log(log_filename)\n",
    "    tracks = [list(track.sort_values(by=[\"timest\"], ascending=True).activity.values) for track_id, track in df.groupby(df.trace)]    \n",
    "    tokens = sorted(list(set(df.activity)) +['#'])\n",
    "    tokens\n",
    "    token_to_id = {t:i for i,t in enumerate(tokens)}\n",
    "    id_to_token = {i:t for i,t in enumerate(tokens)}\n",
    "    MAX_LEN = max(list(map(len, tracks)))\n",
    "    tracks_ix = list(map(lambda track: list(map(token_to_id.get,track)), tracks))\n",
    "    for i in range(len(tracks_ix)):\n",
    "        if len(tracks_ix[i]) < MAX_LEN:\n",
    "            tracks_ix[i] += [token_to_id['#']]*(MAX_LEN - len(tracks_ix[i]))\n",
    "    tracks_ix = np.array(tracks_ix)\n",
    "    return tokens, tracks_ix, id_to_token, token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size):\n",
    "    rows = data[np.random.randint(0,len(data),size=batch_size)]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_state_to_id(binary_state):\n",
    "    return str(int(sum(val*2**index for index, val in enumerate(binary_state))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(tokens, token_to_id, neurons_num=5):\n",
    "    sequence = T.matrix('token sequence','int64')\n",
    "    inputs = sequence[:,:-1] # по символу предсказываю следующий, поэтому входные - все, кроме последнего\n",
    "    targets = sequence[:,1:] # а выходные - все, кроме первого\n",
    "    l_input_sequence = InputLayer(shape=(None, None),input_var=inputs)\n",
    "    tau = theano.shared(np.float32(0.1))\n",
    "    pseudo_sigmoid = GumbelSigmoid(t=tau)\n",
    "    class step:    \n",
    "        #inputs\n",
    "        h_prev = InputLayer((None, neurons_num),name='previous rnn state')\n",
    "        inp = InputLayer((None,),name='current character')\n",
    "        emb = EmbeddingLayer(inp, len(tokens), 30, name='emb') # сопоставление при условии минимума фции потерь\n",
    "        \n",
    "        #recurrent part                 \n",
    "        f_dense = DenseLayer(concat([h_prev, emb]), num_units=8, nonlinearity=T.nnet.relu)    # 0 .. +inf\n",
    "        s_dense = DenseLayer(f_dense, num_units=neurons_num, nonlinearity=None) # -inf .. +inf\n",
    "        \n",
    "        next_state_probs = NonlinearityLayer(s_dense, T.nnet.sigmoid) # Вероятность битов состояния\n",
    "        h_new = NonlinearityLayer(s_dense, pseudo_sigmoid) # Новое состояние - вектор битов\n",
    "\n",
    "        next_token_probas = DenseLayer(s_dense, len(tokens),nonlinearity=T.nnet.softmax) # Вероятность токена - P(Ti|Ti-1....T0)\n",
    "    batch_size = sequence.shape[0]\n",
    "    initial_state = InputLayer((None, neurons_num), T.zeros((batch_size, neurons_num)))\n",
    "    training_loop = Recurrence(\n",
    "        state_variables={step.h_new:step.h_prev},\n",
    "        state_init={step.h_new:initial_state},\n",
    "        input_sequences={step.inp:l_input_sequence},\n",
    "        tracked_outputs=[step.next_token_probas,],\n",
    "        unroll_scan=False,\n",
    "    )  \n",
    "    weights = lasagne.layers.get_all_params(training_loop, trainable=True)    \n",
    "    predicted_probabilities = lasagne.layers.get_output(training_loop[step.next_token_probas])\n",
    "    xent = lasagne.objectives.categorical_crossentropy(predicted_probabilities.reshape((-1,len(tokens))),\n",
    "                                                   targets.reshape((-1,))).reshape(targets.shape)\n",
    "    mask = T.neq(inputs, token_to_id[\"#\"]) # оставляем только значимые токены, т.к. незначимые появились после паддинга\n",
    "    loss = (mask * xent).sum(axis=1).mean() # функция ошибки\n",
    "\n",
    "    #<Loss function - a simple categorical crossentropy will do, maybe add some regularizer>\n",
    "    updates = lasagne.updates.adam(loss, weights)\n",
    "    train_step = theano.function([sequence], loss,\n",
    "                             updates=training_loop.get_automatic_updates()+updates)\n",
    "    \n",
    "    h_deterministic = NonlinearityLayer(step.next_state_probs, lambda x: T.gt(x, 0.5).astype(x.dtype))\n",
    "    validation_loop = Recurrence(\n",
    "        state_variables={h_deterministic:step.h_prev},\n",
    "        state_init={h_deterministic:initial_state},\n",
    "        input_sequences={step.inp:l_input_sequence},\n",
    "        tracked_outputs=[step.next_token_probas,],\n",
    "        unroll_scan=False,\n",
    "    )\n",
    "    states_seq = get_output(validation_loop[h_deterministic], {l_input_sequence:sequence})\n",
    "    infer_states = theano.function([sequence], states_seq, updates=None)\n",
    "    return train_step, infer_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(tokens, tracks_ix, token_to_id, neurons_num=5, n_epochs = 25, batches_per_epoch = 250, batch_size= 10):\n",
    "    train_step, infer_states = model(tokens,token_to_id)    \n",
    "    for epoch in range(n_epochs):\n",
    "        avg_cost = 0;\n",
    "        for _ in range(batches_per_epoch):\n",
    "            avg_cost += train_step(sample_batch(tracks_ix, batch_size))\n",
    "        print(\"\\n\\nEpoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "    return infer_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_json(filename, inferred_states, tracks_ix, neurons_num=10): \n",
    "    graph = {\"states\":set(), \"transitions\":list(), \"meta\":{\"isAccepting\":set()}}\n",
    "    all_states = []\n",
    "    all_used_states = set()    \n",
    "\n",
    "    for binary_states in inferred_states:\n",
    "        binary_states = np.vstack((np.zeros(neurons_num), binary_states)) # add initial state\n",
    "        states = list(map(binary_state_to_id, binary_states))\n",
    "        graph[\"states\"].update(set(states))\n",
    "        all_states.append(states)    \n",
    "    \n",
    "    for states, track_ids in zip(all_states, tracks_ix):\n",
    "        track = list(map(id_to_token.get, track_ids))\n",
    "        for index, symbol in enumerate(track):\n",
    "            transition = {\"from\":states[index], \"to\":states[index + 1], \"track\":symbol}\n",
    "            all_used_states.add(transition[\"from\"])\n",
    "            all_used_states.add(transition[\"to\"])\n",
    "            if transition not in graph[\"transitions\"]:\n",
    "                graph[\"transitions\"].append(transition)  \n",
    "        graph[\"meta\"][\"isAccepting\"].add(states[len(track)])\n",
    "        \n",
    "    graph[\"states\"] = list(all_used_states)\n",
    "    graph[\"meta\"][\"isAccepting\"] = list(graph[\"meta\"][\"isAccepting\"])\n",
    "    graph[\"meta\"][\"tracksNum\"] = len(tracks_ix)\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        json.dump(graph, json_file)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dot(json_graph, filename):\n",
    "    graph = \"digraph test {\\n\"\n",
    "    for state in json_graph[\"states\"]:\n",
    "        if state in json_graph[\"meta\"][\"isAccepting\"]:\n",
    "            graph += \"\\t\" + state + \" [shape=doublecircle];\\n\"\n",
    "        else:\n",
    "            graph += \"\\t\" + state + \";\\n\"\n",
    "    for transition in json_graph[\"transitions\"]:\n",
    "        graph += \"\\t\" + transition[\"from\"] + \" -> \" + transition[\"to\"] \n",
    "        graph += \" [label=\\\"\" + transition[\"track\"] + \"\\\"];\\n\"\n",
    "    graph += \"}\" \n",
    "    \n",
    "    with open(filename, \"w\") as graph_file:\n",
    "        print(graph, file=graph_file, end=\"\")\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0 average loss = 3.962332988550206\n",
      "\n",
      "\n",
      "Epoch 1 average loss = 2.136803028987426\n",
      "\n",
      "\n",
      "Epoch 2 average loss = 1.86175525344277\n",
      "\n",
      "\n",
      "Epoch 3 average loss = 1.6590040576598115\n",
      "\n",
      "\n",
      "Epoch 4 average loss = 1.5371119987343587\n",
      "\n",
      "\n",
      "Epoch 5 average loss = 1.4736885807658635\n",
      "\n",
      "\n",
      "Epoch 6 average loss = 1.425573951678219\n",
      "\n",
      "\n",
      "Epoch 7 average loss = 1.4257997702321756\n",
      "\n",
      "\n",
      "Epoch 8 average loss = 1.3781801830865053\n",
      "\n",
      "\n",
      "Epoch 9 average loss = 1.3407805026852506\n",
      "\n",
      "\n",
      "Epoch 10 average loss = 1.2271034258780726\n",
      "\n",
      "\n",
      "Epoch 11 average loss = 1.1174788308087285\n",
      "\n",
      "\n",
      "Epoch 12 average loss = 1.002400049335712\n",
      "\n",
      "\n",
      "Epoch 13 average loss = 0.9117776113924964\n",
      "\n",
      "\n",
      "Epoch 14 average loss = 0.8668075448652973\n",
      "\n",
      "\n",
      "Epoch 15 average loss = 0.8211766729082657\n",
      "\n",
      "\n",
      "Epoch 16 average loss = 0.782874127679791\n",
      "\n",
      "\n",
      "Epoch 17 average loss = 0.7636623948403101\n",
      "\n",
      "\n",
      "Epoch 18 average loss = 0.7569500446364912\n",
      "\n",
      "\n",
      "Epoch 19 average loss = 0.7477379331150648\n",
      "\n",
      "\n",
      "Epoch 20 average loss = 0.7429234214451412\n",
      "\n",
      "\n",
      "Epoch 21 average loss = 0.7314433767337961\n",
      "\n",
      "\n",
      "Epoch 22 average loss = 0.725800528979691\n",
      "\n",
      "\n",
      "Epoch 23 average loss = 0.724223474400386\n",
      "\n",
      "\n",
      "Epoch 24 average loss = 0.7206112678978196\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 10 and the array at index 1 has size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-dc05b347d05c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minfer_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtracks_ix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_to_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minferred_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfer_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtracks_ix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtracks_ix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdot_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdot_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\9016danilov-as\\\\Documents\\\\graphviz-2.38\\\\release\\\\bin\\\\dot output/log1.dot -Tpng -o output/log1.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-f45536c36fdd>\u001b[0m in \u001b[0;36mbuild_json\u001b[1;34m(filename, inferred_states, tracks_ix, neurons_num)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbinary_states\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minferred_states\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mbinary_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneurons_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# add initial state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_state_to_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"states\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\9016danilov-as\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 10 and the array at index 1 has size 5"
     ]
    }
   ],
   "source": [
    "tokens, tracks_ix, id_to_token, token_to_id = preprocessing(filename)    \n",
    "infer_states = training(tokens, tracks_ix, token_to_id, 10)\n",
    "inferred_states = infer_states(tracks_ix)\n",
    "graph = build_json(json_filename, inferred_states, tracks_ix)\n",
    "dot_graph = build_dot(graph, dot_filename)\n",
    "!C:\\Users\\9016danilov-as\\Documents\\graphviz-2.38\\release\\bin\\dot output/log1.dot -Tpng -o output/log1.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
