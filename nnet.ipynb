{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.layers import Layer\n",
    "from lasagne.random import get_rng\n",
    "\n",
    "from agentnet import Recurrence\n",
    "from agentnet.resolver import ProbabilisticResolver\n",
    "from agentnet.memory import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'log1'\n",
    "log_filename = 'data/'+filename+'.csv'\n",
    "json_filename = 'output/' + filename + \".json\"\n",
    "dot_filename = 'output/' + filename + \".dot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSigmoid:\n",
    "    def __init__(self,\n",
    "                 t=0.1,\n",
    "                 discrete=True,\n",
    "                 eps=1e-20):\n",
    "        assert t != 0\n",
    "        self.temperature=t\n",
    "        self.eps=eps\n",
    "        self.discrete=discrete\n",
    "        self._srng = RandomStreams(get_rng().randint(1, 2147462579))\n",
    "    def __call__(self,logits):\n",
    "        \"\"\"computes a gumbel softmax sample\"\"\"\n",
    "                \n",
    "        #sample from Gumbel(0, 1)\n",
    "        uniform1 = self._srng.uniform(logits.shape,low=0,high=1)\n",
    "        uniform2 = self._srng.uniform(logits.shape,low=0,high=1)\n",
    "        \n",
    "        noise = -T.log(T.log(uniform2 + self.eps)/T.log(uniform1 + self.eps) +self.eps)\n",
    "        \n",
    "        #draw a sample from the Gumbel-Sigmoid distribution\n",
    "        gumbel_sigm = T.nnet.sigmoid((logits + noise) / self.temperature)\n",
    "        \n",
    "        if self.discrete:\n",
    "            return theano.gradient.zero_grad(hard_sigm(logits + noise) - gumbel_sigm) + gumbel_sigm\n",
    "        else:\n",
    "            return gumbel_sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_sigm(logits):\n",
    "    \"\"\"computes a hard indicator function. Not differentiable\"\"\"\n",
    "    return T.switch(T.gt(logits,0),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log(filename, sep=';', sheetname='Лист1'):\n",
    "    if filename.lower().endswith('.csv'):\n",
    "        df = pd.read_csv(filename,sep=sep)\n",
    "    elif filename.lower().endswith('.xlsx'):\n",
    "        xl = pd.ExcelFile(filename)\n",
    "        df = xl.parse(sheetname)\n",
    "    else:\n",
    "        raise Exception(\"Некорректный тип лога\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(filename):\n",
    "    df = read_log(log_filename)\n",
    "    tracks = [list(track.sort_values(by=[\"timest\"], ascending=True).activity.values) for track_id, track in df.groupby(df.trace)]    \n",
    "    tokens = sorted(list(set(df.activity)) +['#'])\n",
    "    tokens\n",
    "    token_to_id = {t:i for i,t in enumerate(tokens)}\n",
    "    id_to_token = {i:t for i,t in enumerate(tokens)}\n",
    "    MAX_LEN = max(list(map(len, tracks)))\n",
    "    tracks_ix = list(map(lambda track: list(map(token_to_id.get,track)), tracks))\n",
    "    for i in range(len(tracks_ix)):\n",
    "        if len(tracks_ix[i]) < MAX_LEN:\n",
    "            tracks_ix[i] += [token_to_id['#']]*(MAX_LEN - len(tracks_ix[i]))\n",
    "    tracks_ix = np.array(tracks_ix)\n",
    "    return tokens, tracks_ix, id_to_token, token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size):\n",
    "    rows = data[np.random.randint(0,len(data),size=batch_size)]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_state_to_id(binary_state):\n",
    "    return str(int(sum(val*2**index for index, val in enumerate(binary_state))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(tokens, token_to_id, neurons_num=5):\n",
    "    sequence = T.matrix('token sequence','int64')\n",
    "    inputs = sequence[:,:-1] # по символу предсказываю следующий, поэтому входные - все, кроме последнего\n",
    "    targets = sequence[:,1:] # а выходные - все, кроме первого\n",
    "    l_input_sequence = InputLayer(shape=(None, None),input_var=inputs)\n",
    "    tau = theano.shared(np.float32(0.1))\n",
    "    pseudo_sigmoid = GumbelSigmoid(t=tau)\n",
    "    class step:    \n",
    "        #inputs\n",
    "        h_prev = InputLayer((None, neurons_num),name='previous rnn state')\n",
    "        inp = InputLayer((None,),name='current character')\n",
    "        emb = EmbeddingLayer(inp, len(tokens), 30, name='emb') # сопоставление при условии минимума фции потерь\n",
    "        \n",
    "        #recurrent part                 \n",
    "        f_dense = DenseLayer(concat([h_prev, emb]), num_units=8, nonlinearity=T.nnet.relu)    # 0 .. +inf\n",
    "        s_dense = DenseLayer(f_dense, num_units=neurons_num, nonlinearity=None) # -inf .. +inf\n",
    "        \n",
    "        next_state_probs = NonlinearityLayer(s_dense, T.nnet.sigmoid) # Вероятность битов состояния\n",
    "        h_new = NonlinearityLayer(s_dense, pseudo_sigmoid) # Новое состояние - вектор битов\n",
    "\n",
    "        next_token_probas = DenseLayer(s_dense, len(tokens),nonlinearity=T.nnet.softmax) # Вероятность токена - P(Ti|Ti-1....T0)\n",
    "    batch_size = sequence.shape[0]\n",
    "    initial_state = InputLayer((None, neurons_num), T.zeros((batch_size, neurons_num)))\n",
    "    training_loop = Recurrence(\n",
    "        state_variables={step.h_new:step.h_prev},\n",
    "        state_init={step.h_new:initial_state},\n",
    "        input_sequences={step.inp:l_input_sequence},\n",
    "        tracked_outputs=[step.next_token_probas,],\n",
    "        unroll_scan=False,\n",
    "    )  \n",
    "    weights = lasagne.layers.get_all_params(training_loop, trainable=True)    \n",
    "    predicted_probabilities = lasagne.layers.get_output(training_loop[step.next_token_probas])\n",
    "    xent = lasagne.objectives.categorical_crossentropy(predicted_probabilities.reshape((-1,len(tokens))),\n",
    "                                                   targets.reshape((-1,))).reshape(targets.shape)\n",
    "    mask = T.neq(inputs, token_to_id[\"#\"]) # оставляем только значимые токены, т.к. незначимые появились после паддинга\n",
    "    loss = (mask * xent).sum(axis=1).mean() # функция ошибки\n",
    "\n",
    "    #<Loss function - a simple categorical crossentropy will do, maybe add some regularizer>\n",
    "    updates = lasagne.updates.adam(loss, weights)\n",
    "    train_step = theano.function([sequence], loss,\n",
    "                             updates=training_loop.get_automatic_updates()+updates)\n",
    "    \n",
    "    h_deterministic = NonlinearityLayer(step.next_state_probs, lambda x: T.gt(x, 0.5).astype(x.dtype))\n",
    "    validation_loop = Recurrence(\n",
    "        state_variables={h_deterministic:step.h_prev},\n",
    "        state_init={h_deterministic:initial_state},\n",
    "        input_sequences={step.inp:l_input_sequence},\n",
    "        tracked_outputs=[step.next_token_probas,],\n",
    "        unroll_scan=False,\n",
    "    )\n",
    "    states_seq = get_output(validation_loop[h_deterministic], {l_input_sequence:sequence})\n",
    "    infer_states = theano.function([sequence], states_seq, updates=None)\n",
    "    return train_step, infer_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(tokens, tracks_ix, token_to_id, neurons_num=5, n_epochs = 25, batches_per_epoch = 250, batch_size= 10):\n",
    "    train_step, infer_states = model(tokens,token_to_id)    \n",
    "    for epoch in range(n_epochs):\n",
    "        avg_cost = 0;\n",
    "        for _ in range(batches_per_epoch):\n",
    "            avg_cost += train_step(sample_batch(tracks_ix, batch_size))\n",
    "        print(\"\\n\\nEpoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "    return infer_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_json(filename, inferred_states, tracks_ix, neurons_num=10): \n",
    "    graph = {\"states\":set(), \"transitions\":list(), \"meta\":{\"isAccepting\":set()}}\n",
    "    all_states = []\n",
    "    all_used_states = set()    \n",
    "\n",
    "    for binary_states in inferred_states:\n",
    "        binary_states = np.vstack((np.zeros(neurons_num), binary_states)) # add initial state\n",
    "        states = list(map(binary_state_to_id, binary_states))\n",
    "        graph[\"states\"].update(set(states))\n",
    "        all_states.append(states)    \n",
    "    \n",
    "    for states, track_ids in zip(all_states, tracks_ix):\n",
    "        track = list(map(id_to_token.get, track_ids))\n",
    "        for index, symbol in enumerate(track):\n",
    "            transition = {\"from\":states[index], \"to\":states[index + 1], \"track\":symbol}\n",
    "            all_used_states.add(transition[\"from\"])\n",
    "            all_used_states.add(transition[\"to\"])\n",
    "            if transition not in graph[\"transitions\"]:\n",
    "                graph[\"transitions\"].append(transition)  \n",
    "        graph[\"meta\"][\"isAccepting\"].add(states[len(track)])\n",
    "        \n",
    "    graph[\"states\"] = list(all_used_states)\n",
    "    graph[\"meta\"][\"isAccepting\"] = list(graph[\"meta\"][\"isAccepting\"])\n",
    "    graph[\"meta\"][\"tracksNum\"] = len(tracks_ix)\n",
    "    with open(filename, \"w\") as json_file:\n",
    "        json.dump(graph, json_file)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dot(json_graph, filename):\n",
    "    graph = \"digraph test {\\n\"\n",
    "    for state in json_graph[\"states\"]:\n",
    "        if state in json_graph[\"meta\"][\"isAccepting\"]:\n",
    "            graph += \"\\t\" + state + \" [shape=doublecircle];\\n\"\n",
    "        else:\n",
    "            graph += \"\\t\" + state + \";\\n\"\n",
    "    for transition in json_graph[\"transitions\"]:\n",
    "        graph += \"\\t\" + transition[\"from\"] + \" -> \" + transition[\"to\"] \n",
    "        graph += \" [label=\\\"\" + transition[\"track\"] + \"\\\"];\\n\"\n",
    "    graph += \"}\" \n",
    "    \n",
    "    with open(filename, \"w\") as graph_file:\n",
    "        print(graph, file=graph_file, end=\"\")\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, tracks_ix, id_to_token, token_to_id = preprocessing(filename)    \n",
    "infer_states = training(tokens, tracks_ix, token_to_id, 10)\n",
    "inferred_states = infer_states(tracks_ix)\n",
    "graph = build_json(json_filename, inferred_states, tracks_ix)\n",
    "dot_graph = build_dot(graph, dot_filename)\n",
    "#!dot output/log1.dot -Tpng -o output/log1.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
